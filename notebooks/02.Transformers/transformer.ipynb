{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/xuyijie/anaconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/data/xuyijie/Documents/AllsidesScraper/news-title-bias/data/dataset/dataset.csv', header=0, encoding='utf_8_sig')\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train.to_csv('/home/data/xuyijie/Documents/AllsidesScraper/news-title-bias/data/dataset/train.csv', index=False, encoding='utf-8')\n",
    "test.to_csv('/home/data/xuyijie/Documents/AllsidesScraper/news-title-bias/data/dataset/test.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-463d696f02a7192d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/yijiexu/.cache/huggingface/datasets/text/default-463d696f02a7192d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066f52e791df449a88045ff04c593341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1edb7685344fe98362c2d39efec67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a10c07a01cf4a039b81907b8bdf837a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0915dcb77f4772868062d4efc1827f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/yijiexu/.cache/huggingface/datasets/text/default-463d696f02a7192d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b437a8a654ea46e9aeb39fadf6477f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('text', data_files={'train': '../data/dataset/train.csv', 'test': '../data/dataset/test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 214855\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 23893\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9b1c84ede94d9b9f70dc140fbd7c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/215 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\n",
      "2\n",
      "1\n",
      "-2\n",
      "2\n",
      "-1\n",
      "-1\n",
      "2\n",
      "-1\n",
      "-2\n",
      "1\n",
      "-1\n",
      "-1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     tokenized[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m labels\n\u001b[1;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenized\n\u001b[0;32m---> 13\u001b[0m tokenized_datasets \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(tokenize_function, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     14\u001b[0m train_dataset \u001b[39m=\u001b[39m tokenized_datasets[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshuffle(seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m     15\u001b[0m eval_dataset \u001b[39m=\u001b[39m tokenized_datasets[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshuffle(seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/datasets/dataset_dict.py:816\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    814\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    815\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 816\u001b[0m     {\n\u001b[1;32m    817\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    818\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[1;32m    819\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[1;32m    820\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[1;32m    821\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[1;32m    822\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[1;32m    823\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    824\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    825\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[1;32m    826\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    827\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    828\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    829\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    830\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[1;32m    831\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[1;32m    832\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[1;32m    833\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[1;32m    834\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    837\u001b[0m     }\n\u001b[1;32m    838\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/datasets/dataset_dict.py:817\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    814\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    815\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    816\u001b[0m     {\n\u001b[0;32m--> 817\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m    818\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m    819\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m    820\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m    821\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m    822\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m    823\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    824\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m    825\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m    826\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m    827\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m    828\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[1;32m    829\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m    830\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    831\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m    832\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m    833\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    834\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    837\u001b[0m     }\n\u001b[1;32m    838\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:2826\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2823\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   2825\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2826\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[1;32m   2827\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m   2828\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m   2829\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m   2830\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m   2831\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m   2832\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2833\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m   2834\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m   2835\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   2836\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m   2837\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[1;32m   2838\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   2839\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   2840\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m   2841\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m   2842\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   2843\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[1;32m   2844\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m   2845\u001b[0m     )\n\u001b[1;32m   2846\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2848\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    561\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    563\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    564\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    565\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:529\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    522\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    523\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    524\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    525\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    526\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    527\u001b[0m }\n\u001b[1;32m    528\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 529\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    530\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    531\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/datasets/fingerprint.py:480\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    478\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    482\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:3247\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   3243\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   3244\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(input_dataset\u001b[39m.\u001b[39mnum_rows)))\n\u001b[1;32m   3245\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3246\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3247\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   3248\u001b[0m         batch,\n\u001b[1;32m   3249\u001b[0m         indices,\n\u001b[1;32m   3250\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(input_dataset\u001b[39m.\u001b[39mlist_indexes()) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[1;32m   3251\u001b[0m         offset\u001b[39m=\u001b[39moffset,\n\u001b[1;32m   3252\u001b[0m     )\n\u001b[1;32m   3253\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3254\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3255\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3256\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:3123\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3121\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3122\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3123\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3124\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3125\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3126\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3127\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[40], line 6\u001b[0m, in \u001b[0;36mtokenize_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m examples[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m      5\u001b[0m     split \u001b[39m=\u001b[39m example\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, maxsplit\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mprint\u001b[39m(split[\u001b[39m1\u001b[39;49m])\n\u001b[1;32m      7\u001b[0m     labels\u001b[39m.\u001b[39mappend(\u001b[39mint\u001b[39m(split[\u001b[39m1\u001b[39m]))\n\u001b[1;32m      8\u001b[0m     texts\u001b[39m.\u001b[39mappend(split[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for example in examples['text']:\n",
    "        split = example.split(',', maxsplit=1)\n",
    "        print(split[1])\n",
    "        labels.append(int(split[1]))\n",
    "        texts.append(split[0])\n",
    "    tokenized = tokenizer(texts, padding='max_length', truncation=True, max_length=32)\n",
    "    tokenized['labels'] = labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Benjamin Netanyahu officially informed Israels President that he has successfully formed a new government some six weeks after the Knesset elections'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text'][18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file, header=0)\n",
    "                                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        sample = {'text': row[0], 'label': row[1]}\n",
    "        return sample\n",
    "\n",
    "dataset = CustomDataset(\"../data/dataset/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import gc\n",
    "from scipy.special import softmax\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold \n",
    "import sklearn\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "#choose the same seed to assure that our model will be roproducible\n",
    "\n",
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value) # Python\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    torch.manual_seed(seed_value) # cpu  vars\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  #needed\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_all(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#We consider that our data is a csv file (2 columns : text and label)\n",
    "#using pandas function (read_csv) to read the file\n",
    "train=pd.read_csv('../data/dataset/dataset.csv', encoding='utf_8_sig', header=0)\n",
    "\n",
    "feat_cols = \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, -1,  2, -2,  1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['Lean Left', 'Lean Right', 'Center', 'Left', 'Right']\n",
    "train['label']=train.label.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = to_categorical(train.label.astype('category'), num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [-2,-1,0,1,2]\n",
    "for i in range(len(label_cols)) :     \n",
    "     train[l[i]] = Y[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>-2</th>\n",
       "      <th>-1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summary U.S. announces new $1.85 bln military aid package for Kyiv  Zelenskiy's first trip abroad since Russian invasion began  U.S. Patriot air defence battery part of new assistance  Putin backs...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In an extraordinary face-to-face meeting at the White House on Wednesday, President Joe Biden told Ukrainian President Volodymyr Zelenskyy that the U.S. \"will stay with you as long as it takes\" as...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ukrainian President Volodymyr Zelenskyy on Wednesday thanked the U.S. for its support for his nation and said he wants Americans to keep working together to defeat Russia's aspirations to control ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A patient is turned away from the emergency room due to full capacity at the Baoding No. 2 Central Hospital in Zhuozhou city in northern China's Hebei province on Wednesday, Dec. 21, 2022. China o...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Washington CNN —  Two Americans who had been detained by the Taliban in Afghanistan have been released and are en route to Qatar, three sources familiar with the matter tell CNN.  Two of the sourc...</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233565</th>\n",
       "      <td>Comment on this story Comment Gift Article Share  Hiring surged and wages grew more than they have in almost a decade, the government said Friday in a report seized on by Republicans just before t...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233566</th>\n",
       "      <td>Early withdrawals from a 401(k) typically come with a 10 percent tax. Under the new proposal, a person would be able to make one penalty-free withdrawal for unexpected or immediate expenses arisin...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233567</th>\n",
       "      <td>Listen Comment on this story Comment Gift Article Share  The Senate on Tuesday took the first formal step toward advancing a bipartisan, roughly $1.7 trillion deal to fund the U.S. government, as ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233568</th>\n",
       "      <td>Sen. Elizabeth Warren (D-Mass.) said Federal Reserve chair Jerome Powell is \"pushing hard to get more people fired because he thinks that is one way to help bring down inflation.\" Pool via Getty I...</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233569</th>\n",
       "      <td>At this time last year, economists were hopeful that snarls in global shipping and manufacturing would soon clear; consumer spending would shift away from goods and back to services; and the combi...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>233570 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                           text  \\\n",
       "0       Summary U.S. announces new $1.85 bln military aid package for Kyiv  Zelenskiy's first trip abroad since Russian invasion began  U.S. Patriot air defence battery part of new assistance  Putin backs...   \n",
       "1       In an extraordinary face-to-face meeting at the White House on Wednesday, President Joe Biden told Ukrainian President Volodymyr Zelenskyy that the U.S. \"will stay with you as long as it takes\" as...   \n",
       "2       Ukrainian President Volodymyr Zelenskyy on Wednesday thanked the U.S. for its support for his nation and said he wants Americans to keep working together to defeat Russia's aspirations to control ...   \n",
       "3       A patient is turned away from the emergency room due to full capacity at the Baoding No. 2 Central Hospital in Zhuozhou city in northern China's Hebei province on Wednesday, Dec. 21, 2022. China o...   \n",
       "4       Washington CNN —  Two Americans who had been detained by the Taliban in Afghanistan have been released and are en route to Qatar, three sources familiar with the matter tell CNN.  Two of the sourc...   \n",
       "...                                                                                                                                                                                                         ...   \n",
       "233565  Comment on this story Comment Gift Article Share  Hiring surged and wages grew more than they have in almost a decade, the government said Friday in a report seized on by Republicans just before t...   \n",
       "233566  Early withdrawals from a 401(k) typically come with a 10 percent tax. Under the new proposal, a person would be able to make one penalty-free withdrawal for unexpected or immediate expenses arisin...   \n",
       "233567  Listen Comment on this story Comment Gift Article Share  The Senate on Tuesday took the first formal step toward advancing a bipartisan, roughly $1.7 trillion deal to fund the U.S. government, as ...   \n",
       "233568  Sen. Elizabeth Warren (D-Mass.) said Federal Reserve chair Jerome Powell is \"pushing hard to get more people fired because he thinks that is one way to help bring down inflation.\" Pool via Getty I...   \n",
       "233569  At this time last year, economists were hopeful that snarls in global shipping and manufacturing would soon clear; consumer spending would shift away from goods and back to services; and the combi...   \n",
       "\n",
       "       label  -2  -1  0  1  2  \n",
       "0          0   1   0  0  0  0  \n",
       "1         -1   0   0  0  0  1  \n",
       "2          2   0   0  1  0  0  \n",
       "3         -1   0   0  0  0  1  \n",
       "4         -2   0   0  0  1  0  \n",
       "...      ...  ..  .. .. .. ..  \n",
       "233565    -1   0   0  0  0  1  \n",
       "233566    -1   0   0  0  0  1  \n",
       "233567    -1   0   0  0  0  1  \n",
       "233568    -2   0   0  0  1  0  \n",
       "233569    -1   0   0  0  0  1  \n",
       "\n",
       "[233570 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/conda-bld/pytorch_1670525541990/work/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525541990/work/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525541990/work/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525541990/work/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1670525541990/work/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 30\u001b[0m\n\u001b[1;32m      7\u001b[0m train1_trn, train1_val \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39miloc[train_index], train\u001b[39m.\u001b[39miloc[test_index]\n\u001b[1;32m      8\u001b[0m model \u001b[39m=\u001b[39m ClassificationModel(\n\u001b[1;32m      9\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mroberta\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mroberta-base\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     },\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m model\u001b[39m.\u001b[39;49mtrain_model(train1_trn)\n\u001b[1;32m     31\u001b[0m raw_outputs_val \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval_model(train1_val)[\u001b[39m1\u001b[39m]\n\u001b[1;32m     32\u001b[0m raw_outputs_vals \u001b[39m=\u001b[39m softmax(raw_outputs_val, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:632\u001b[0m, in \u001b[0;36mClassificationModel.train_model\u001b[0;34m(self, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m    624\u001b[0m     train_dataset,\n\u001b[1;32m    625\u001b[0m     sampler\u001b[39m=\u001b[39mtrain_sampler,\n\u001b[1;32m    626\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtrain_batch_size,\n\u001b[1;32m    627\u001b[0m     num_workers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdataloader_num_workers,\n\u001b[1;32m    628\u001b[0m )\n\u001b[1;32m    630\u001b[0m os\u001b[39m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 632\u001b[0m global_step, training_details \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m    633\u001b[0m     train_dataloader,\n\u001b[1;32m    634\u001b[0m     output_dir,\n\u001b[1;32m    635\u001b[0m     multi_label\u001b[39m=\u001b[39;49mmulti_label,\n\u001b[1;32m    636\u001b[0m     show_running_loss\u001b[39m=\u001b[39;49mshow_running_loss,\n\u001b[1;32m    637\u001b[0m     eval_df\u001b[39m=\u001b[39;49meval_df,\n\u001b[1;32m    638\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    639\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    640\u001b[0m )\n\u001b[1;32m    642\u001b[0m \u001b[39m# model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[39m# model_to_save.save_pretrained(output_dir)\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[39m# self.tokenizer.save_pretrained(output_dir)\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[39m# torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_model(model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:929\u001b[0m, in \u001b[0;36mClassificationModel.train\u001b[0;34m(self, train_dataloader, output_dir, multi_label, show_running_loss, eval_df, test_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    925\u001b[0m     loss \u001b[39m=\u001b[39m (\n\u001b[1;32m    926\u001b[0m         loss\u001b[39m.\u001b[39mmean()\n\u001b[1;32m    927\u001b[0m     )  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n\u001b[0;32m--> 929\u001b[0m current_loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m    931\u001b[0m \u001b[39mif\u001b[39;00m show_running_loss:\n\u001b[1;32m    932\u001b[0m     batch_iterator\u001b[39m.\u001b[39mset_description(\n\u001b[1;32m    933\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpochs \u001b[39m\u001b[39m{\u001b[39;00mepoch_number\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m.\u001b[39mnum_train_epochs\u001b[39m}\u001b[39;00m\u001b[39m. Running Loss: \u001b[39m\u001b[39m{\u001b[39;00mcurrent_loss\u001b[39m:\u001b[39;00m\u001b[39m9.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "err = []\n",
    "y_pred_tot = []\n",
    "\n",
    "fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1997)\n",
    "i = 1\n",
    "for train_index, test_index in fold.split(train, train[\"label\"]):\n",
    "    train1_trn, train1_val = train.iloc[train_index], train.iloc[test_index]\n",
    "    model = ClassificationModel(\n",
    "        \"roberta\",\n",
    "        \"roberta-base\",\n",
    "        use_cuda=True,\n",
    "        num_labels=4,\n",
    "        args={\n",
    "            \"train_batch_size\": 16,\n",
    "            \"reprocess_input_data\": True,\n",
    "            \"overwrite_output_dir\": True,\n",
    "            \"fp16\": False,\n",
    "            \"do_lower_case\": False,\n",
    "            \"num_train_epochs\": 4,\n",
    "            \"max_seq_length\": 128,\n",
    "            \"regression\": False,\n",
    "            \"manual_seed\": 1997,\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"weight_decay\": 0,\n",
    "            \"save_eval_checkpoints\": True,\n",
    "            \"save_model_every_epoch\": False,\n",
    "            \"silent\": True,\n",
    "        },\n",
    "    )\n",
    "    model.train_model(train1_trn)\n",
    "    raw_outputs_val = model.eval_model(train1_val)[1]\n",
    "    raw_outputs_vals = softmax(raw_outputs_val, axis=1)\n",
    "    print(f\"Log_Loss: {log_loss(train1_val['label'], raw_outputs_vals)}\")\n",
    "    err.append(log_loss(train1_val[\"label\"], raw_outputs_vals))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a6617bc4d31fbf939b961980527353c3bee4d017eb536a18d0f87f30ff59d12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
